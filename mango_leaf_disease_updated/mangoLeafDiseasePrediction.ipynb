{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# âœ… STEP 1: Install & Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests, zipfile, io\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# âœ… STEP 2: Download & Extract the Dataset\n",
    "dataset_url = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/hxsnvwty3r-1.zip\"\n",
    "# This is the base directory where the zip file will be extracted.\n",
    "# The zip file itself contains a top-level folder named \"MangoLeafBD Dataset\".\n",
    "# So, the actual image data will be inside: extracted_base_dir/MangoLeafBD Dataset/\n",
    "extracted_base_dir = \"/content/extracted_dataset\"\n",
    "os.makedirs(extracted_base_dir, exist_ok=True)\n",
    "\n",
    "# Define the actual root directory where the image classes are located after extraction\n",
    "expected_dataset_folder_name = \"MangoLeafBD Dataset\"\n",
    "initial_image_source_dir = os.path.join(extracted_base_dir, expected_dataset_folder_name)\n",
    "\n",
    "# --- NEW ROBUST EXTRACTION LOGIC ---\n",
    "# Always ensure the initial_image_source_dir is populated for splitting.\n",
    "# If it's empty or doesn't exist, re-download and re-extract.\n",
    "# This handles cases where previous runs moved files out.\n",
    "if not os.path.exists(initial_image_source_dir) or not os.listdir(initial_image_source_dir):\n",
    "    print(\"Dataset not found or empty in source directory. Downloading and extracting dataset...\")\n",
    "    # Clean up previous partial extraction attempts if any\n",
    "    if os.path.exists(extracted_base_dir):\n",
    "        shutil.rmtree(extracted_base_dir)\n",
    "    os.makedirs(extracted_base_dir, exist_ok=True) # Recreate the base directory\n",
    "\n",
    "    response = requests.get(dataset_url)\n",
    "    dataset_zip = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "    dataset_zip.extractall(extracted_base_dir) # Extract into the base directory\n",
    "    dataset_zip.close()\n",
    "    print(\"âœ… Dataset downloaded and extracted\")\n",
    "else:\n",
    "    print(\"Dataset already exists in source directory and is populated. Skipping download and extraction.\")\n",
    "\n",
    "# âœ… STEP 3: Organize Dataset into Train & Validation\n",
    "train_dir = \"/content/MangoLeafBD_Train\"\n",
    "val_dir = \"/content/MangoLeafBD_Val\"\n",
    "\n",
    "class_names = [\"Anthracnose\", \"Bacterial Canker\", \"Cutting Weevil\", \"Die Back\", \"Gall Midge\", \"Healthy\", \"Powdery Mildew\", \"Sooty Mould\"]\n",
    "val_split = 0.2\n",
    "\n",
    "# --- NEW ROBUST SPLITTING LOGIC ---\n",
    "# Always perform a fresh split to ensure train_dir and val_dir are correctly populated.\n",
    "# This is necessary because shutil.move empties the source directory each time.\n",
    "print(\"Cleaning up previous train/validation directories for a fresh split...\")\n",
    "if os.path.exists(train_dir):\n",
    "    shutil.rmtree(train_dir)\n",
    "if os.path.exists(val_dir):\n",
    "    shutil.rmtree(val_dir)\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "print(\"Splitting dataset into training and validation sets...\")\n",
    "for class_name in class_names:\n",
    "    class_source_path = os.path.join(initial_image_source_dir, class_name)\n",
    "    train_class_dir = os.path.join(train_dir, class_name)\n",
    "    val_class_dir = os.path.join(val_dir, class_name)\n",
    "\n",
    "    os.makedirs(train_class_dir, exist_ok=True)\n",
    "    os.makedirs(val_class_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Get all image filenames for the current class\n",
    "        images = os.listdir(class_source_path)\n",
    "        # Filter out any non-image files (e.g., .DS_Store, hidden files)\n",
    "        images = [img for img in images if img.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Source directory not found for class: {class_name} at {class_source_path}. This class will be empty in split.\")\n",
    "        continue # Skip to the next class if directory is not found\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while listing files for class {class_name}: {e}. Skipping this class.\")\n",
    "        continue\n",
    "\n",
    "    if not images:\n",
    "        print(f\"No valid images found for class: {class_name} in {class_source_path}. Skipping split for this class.\")\n",
    "        continue\n",
    "\n",
    "    # Split image filenames into training and validation sets\n",
    "    train_images, val_images = train_test_split(images, test_size=val_split, random_state=42)\n",
    "\n",
    "    # Move images to their respective directories\n",
    "    # IMPORTANT: shutil.move moves the files, so the original directory will become empty after this.\n",
    "    for img in train_images:\n",
    "        src_path = os.path.join(class_source_path, img)\n",
    "        dst_path = os.path.join(train_class_dir, img)\n",
    "        if os.path.exists(src_path): # Double-check if source file exists before moving\n",
    "            shutil.move(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"Warning: Source file not found for move to train: {src_path}\")\n",
    "\n",
    "    for img in val_images:\n",
    "        src_path = os.path.join(class_source_path, img)\n",
    "        dst_path = os.path.join(val_class_dir, img)\n",
    "        if os.path.exists(src_path): # Double-check if source file exists before moving\n",
    "            shutil.move(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"Warning: Source file not found for move to val: {src_path}\")\n",
    "print(\"âœ… Dataset split into training and validation\")\n",
    "\n",
    "\n",
    "# âœ… STEP 4: Create Image Generators\n",
    "batch_size = 32\n",
    "image_size = (224, 224) # Standard input size for many pre-trained models, good choice\n",
    "\n",
    "# Data augmentation for training to improve model generalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, # Normalize pixel values to [0, 1]\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest' # Strategy for filling in new pixels created by transformations\n",
    ")\n",
    "\n",
    "# Only rescaling for validation data to ensure consistent evaluation\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators from the newly created train and validation directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical' # Use 'categorical' for one-hot encoded labels (8 classes)\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Corrected attribute from num_samples to samples\n",
    "print(f\"Found {train_generator.samples} training images belonging to {train_generator.num_classes} classes.\")\n",
    "print(f\"Found {val_generator.samples} validation images belonging to {val_generator.num_classes} classes.\")\n",
    "\n",
    "# âœ… STEP 5: Build the Model\n",
    "# Define the input shape for the first convolutional layer\n",
    "input_shape = (image_size[0], image_size[1], 3) # (height, width, channels)\n",
    "\n",
    "model = Sequential([\n",
    "    # First convolutional block\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Second convolutional block\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Third convolutional block\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Fourth convolutional block (added for deeper feature extraction)\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Flatten the 3D output to 1D for the dense layers\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Fully connected layers\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5), # Dropout for regularization to prevent overfitting\n",
    "\n",
    "    # Output layer: 8 units for 8 classes with softmax for probability distribution\n",
    "    layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', # Adam optimizer is a good general-purpose choice\n",
    "              loss='categorical_crossentropy', # Appropriate for multi-class classification with one-hot labels\n",
    "              metrics=['accuracy']) # Monitor accuracy during training\n",
    "\n",
    "model.summary() # Print a summary of the model architecture\n",
    "\n",
    "# âœ… STEP 6: Train the Model\n",
    "epochs = 10 # You can increase to 20+ for better results\n",
    "print(f\"\\nStarting model training for {epochs} epochs...\")\n",
    "# Ensure generators have samples before fitting the model\n",
    "if train_generator.samples == 0 or val_generator.samples == 0:\n",
    "    print(\"Error: No samples found in one or both generators. Cannot train the model.\")\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_generator,\n",
    "        steps_per_epoch=train_generator.samples // train_generator.batch_size, # Calculate steps per epoch\n",
    "        validation_steps=val_generator.samples // val_generator.batch_size # Calculate validation steps\n",
    "    )\n",
    "    print(\"âœ… Model training finished.\")\n",
    "\n",
    "    # âœ… STEP 7: Evaluate and Visualize Results\n",
    "    print(\"\\nEvaluating model performance...\")\n",
    "    train_metrics = model.evaluate(train_generator, steps=train_generator.samples // train_generator.batch_size)\n",
    "    val_metrics = model.evaluate(val_generator, steps=val_generator.samples // val_generator.batch_size)\n",
    "\n",
    "    print(\"\\nðŸ“Š Training Metrics:\")\n",
    "    print(f\"Loss: {train_metrics[0]:.4f} | Accuracy: {train_metrics[1]*100:.2f}%\")\n",
    "    print(\"\\nðŸ“Š Validation Metrics:\")\n",
    "    print(f\"Loss: {val_metrics[0]:.4f} | Accuracy: {val_metrics[1]*100:.2f}%\")\n",
    "\n",
    "    # Plotting training history (loss and accuracy)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Model Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title('Model Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"âœ… Evaluation and visualization complete.\")\n",
    "\n",
    "    # âœ… STEP 8: Save the Model and Download\n",
    "    model_filename = \"mango_leaf_disease_model.h5\"\n",
    "    model.save(model_filename)\n",
    "    print(f\"âœ… Model saved as {model_filename}\")\n",
    "\n",
    "    # This part is specific to Google Colab for downloading the file.\n",
    "    # If you are running this script outside of Google Colab (e.g., on your local machine),\n",
    "    # the 'files.download' line will cause an error.\n",
    "    # In a local environment, the model will simply be saved to the current directory.\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(model_filename)\n",
    "        print(f\"âœ… Model '{model_filename}' downloaded to your local machine.\")\n",
    "    except ImportError:\n",
    "        print(\"\\nNote: 'google.colab.files' not found. This script is likely not running in Google Colab.\")\n",
    "        print(f\"The model '{model_filename}' is saved in the current directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file download: {e}\")\n",
    "    else:\n",
    "        print(\"Model training skipped due to empty generators.\")\n",
    "\n",
    "# Optional: Cleanup the extracted and split datasets if no longer needed\n",
    "# print(\"\\nCleaning up temporary dataset directories...\")\n",
    "# shutil.rmtree(extracted_base_dir)\n",
    "# shutil.rmtree(train_dir)\n",
    "# shutil.rmtree(val_dir)\n",
    "# print(\"âœ… Cleanup complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
